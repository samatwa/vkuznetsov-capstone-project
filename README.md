# Capstone Проєкт: Бенчмарк Функцій Активації

Цей проєкт порівнює 9 функцій активації на 2 архітектурах глибокого навчання:
- **ResNet-34** на **CIFAR-10** (Комп'ютерний зір)
- **Transformer Encoder** на **AG News** (NLP / Обробка природної мови)

## Вимоги

Встановіть залежності:
```bash
pip install -r requirements.txt
```

## Структура Проєкту

- `activations.py`: Користувацькі функції активації (Mish, Swish тощо) та реєстр.
- `models/`: Містить визначення моделей (`resnet.py`, `transformer.py`).
- `data.py`: Скрипти завантаження та попередньої обробки даних.
- `train_utils.py`: Цикл навчання, розрахунок метрик (FLOPs, Пам'ять, Точність).
- `main.py`: Головний скрипт для запуску всіх 18 експериментів.

## Запуск Експериментів

Щоб запустити повний набір експериментів:
```bash
python main.py
```

Це виконає такі дії:
1. Завантажить датасети (CIFAR-10, AG News).
2. Навчить моделі, використовуючи **фіксований поділ Train/Validation/Test (90% Train, 10% Val від офіційного тренувального набору, плюс офіційний тестовий набір)**.
3. Навчатиме протягом **30 епох** для кожного експерименту (зменшено з 50 для ефективності).
4. Збереже контрольні точки (checkpoints) у `checkpoints/` та детальні результати у `results/`.
5. Використовує фіксований random seed (`42`) для повної відтворюваності.

## Метрики
- Точність (Accuracy)
- Втрати на валідації (Validation Loss)
- Час навчання (Training Time)
- FLOPs (Операції з плаваючою комою)
- Норма градієнта (Gradient Norm)
- Пікове використання пам'яті GPU (Peak GPU Memory Usage)

## Порівнювані Функції Активації
- ReLU
- LeakyReLU
- ELU
- SELU
- GELU
- Swish (SiLU)
- Mish
- Hardswish
- Softplus

## Примітки
- Переконайтеся, що CUDA доступна для швидшого навчання.
- Змініть `batch_size` у `main.py`, якщо виникають помилки OOM (недостатньо пам'яті).
- Random seeds зафіксовані на значенні `42` у `main.py` та `train_utils.py`.

